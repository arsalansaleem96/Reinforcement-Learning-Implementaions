{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03a76553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3e1c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58fad205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAvgReward(num_episodes,avg_num_episodes,rewards_all_episodes):\n",
    "        results = []\n",
    "        rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/avg_num_episodes)\n",
    "        count = avg_num_episodes\n",
    "        for r in rewards_per_thousand_episodes:\n",
    "#             avg_per_thousand_episode = str(sum(r/self.avg_num_episodes))\n",
    "            sum_of_rewards = sum(r/avg_num_episodes)\n",
    "            results.append({\"episode\":count,\"reward\":sum_of_rewards})\n",
    "            count += avg_num_episodes\n",
    "        return results\n",
    "    \n",
    "def createLinePlotForRewards(results,avg_num_episodes):\n",
    "        df = pd.DataFrame(results)\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "        sns.lineplot(data=df, x=\"episode\", y=\"reward\", ax=ax1, ci=None)\n",
    "        ax1.set_ylabel('Rewards',fontsize=13)\n",
    "        ax1.set_xlabel('Episodes',fontsize=13)\n",
    "        ax1.set_title(\"Average reward per \"+str(avg_num_episodes)+\" episodes\",fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec9776c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.num_episodes = 10000\n",
    "        self.avg_num_episodes = round(self.num_episodes/10)\n",
    "        self.learning_rate = 0.11\n",
    "        self.discount_rate = 0.99\n",
    "        self.exploration_rate = 1\n",
    "        self.max_exploration_rate = 1\n",
    "        self.min_exploration_rate = 0.0000001\n",
    "        self.exploration_decay_rate = 0.001\n",
    "        self.q_table = self.buidl_q_table()\n",
    "    \n",
    "    def buidl_q_table(self):\n",
    "        action_space_size = env.action_space.n\n",
    "        state_space_size = env.observation_space.n\n",
    "        q_table = np.zeros((state_space_size, action_space_size))\n",
    "        return q_table\n",
    "    \n",
    "    def q_learning(self, ic_count = 1):\n",
    "        rewards_all_episodes = []\n",
    "        \n",
    "\n",
    "        # Q-Learning algorithm\n",
    "        for episode in range(self.num_episodes):\n",
    "            state = env.reset()\n",
    "            clear_output(wait=True)\n",
    "            print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "\n",
    "            done = False\n",
    "            rewards_current_episode = 0\n",
    "            \n",
    "            state_action_pairs = deque()\n",
    "\n",
    "            while not done: \n",
    "\n",
    "                if done == True: \n",
    "                    break\n",
    "                else:\n",
    "                    # Exploration-exploitation trade-off\n",
    "                    self.exploration_rate_threshold = random.uniform(0, 1)\n",
    "                    if self.exploration_rate_threshold > self.exploration_rate:\n",
    "                        action = np.argmax(self.q_table[state,:]) # Exploitation\n",
    "                    else:\n",
    "                        action = env.action_space.sample() # Exploration\n",
    "\n",
    "                    new_state, reward, done, info = env.step(action)\n",
    "                    \n",
    "                    \n",
    "                    state_action_pairs.append((state,new_state,action,reward))\n",
    "                    \n",
    "                    if ic_count < len(state_action_pairs):\n",
    "                        state_action_pairs.popleft()\n",
    "                    \n",
    "                    self.intermittentLearning(state_action_pairs)\n",
    "                    \n",
    "                    state = new_state\n",
    "                    rewards_current_episode += reward\n",
    "\n",
    "            # Exploration rate decay\n",
    "            self.exploration_rate = self.min_exploration_rate + \\\n",
    "                (self.max_exploration_rate - self.min_exploration_rate) * np.exp(-self.exploration_decay_rate*episode)\n",
    "\n",
    "            rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "        results = calculateAvgReward(self.num_episodes,self.avg_num_episodes,rewards_all_episodes)\n",
    "        createLinePlotForRewards(results,self.avg_num_episodes)\n",
    "        \n",
    "    def intermittentLearning(self,state_action_pairs):\n",
    "            \n",
    "        for previous_state,current_state,action,reward in state_action_pairs:\n",
    "\n",
    "            self.sarsaEquation(previous_state,current_state,action,reward)\n",
    "            \n",
    "    def sarsaEquation(self,previous_state,current_state,action,reward):\n",
    "        # Update Q-table for Q(s,a)\n",
    "        self.q_table[previous_state, action] = self.q_table[previous_state, action]  + \\\n",
    "                    self.learning_rate * \\\n",
    "                (reward + self.discount_rate * np.max(self.q_table[current_state, :]) - self.q_table[previous_state, action])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****EPISODE  47 *****\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.q_learning(ic_count = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
