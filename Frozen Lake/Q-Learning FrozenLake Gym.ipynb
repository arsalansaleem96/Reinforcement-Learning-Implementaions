{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fc7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source\n",
    "## https://deeplizard.com/learn/video/QK_PP_2KgGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cdae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b2d8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45d40add",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f7836c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(action_space_size)\n",
    "print(state_space_size)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09cb551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91b0417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(q_table):\n",
    "    for episode in range(3):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "        time.sleep(1)\n",
    "        for step in range(max_steps_per_episode):        \n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            action = np.argmax(q_table[state,:])        \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if done:\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                if reward == 1:\n",
    "                    print(\"****You reached the goal!****\")\n",
    "                    time.sleep(2)\n",
    "                else:\n",
    "                    print(\"****You fell through a hole!****\")\n",
    "                    time.sleep(1)\n",
    "                clear_output(wait=True)\n",
    "                break\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af701756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic(q_table,exploration_rate,num_episodes,learning_rate,discount_rate):\n",
    "    \n",
    "    rewards_all_episodes = []\n",
    "\n",
    "    # Q-Learning algorithm\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode): \n",
    "\n",
    "            # Exploration-exploitation trade-off\n",
    "            exploration_rate_threshold = random.uniform(0, 1)\n",
    "            if exploration_rate_threshold > exploration_rate:\n",
    "                action = np.argmax(q_table[state,:]) # Exploitation\n",
    "            else:\n",
    "                action = env.action_space.sample() # Exploration\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Update Q-table for Q(s,a)\n",
    "            q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "                learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "            state = new_state\n",
    "            rewards_current_episode += reward\n",
    "\n",
    "\n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "\n",
    "        # Exploration rate decay\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "    # Calculate and print the average reward per thousand episodes\n",
    "    rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "    count = 1000\n",
    "\n",
    "    print(\"********Average reward per thousand episodes********\\n\")\n",
    "    for r in rewards_per_thousand_episodes:\n",
    "        print(count, \": \", str(sum(r/1000)))\n",
    "        count += 1000\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d14cd98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(q_table,exploration_rate,num_episodes,learning_rate,discount_rate):\n",
    "    \n",
    "    rewards_all_episodes = []\n",
    "\n",
    "    # Q-Learning algorithm\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode): \n",
    "\n",
    "            # Exploration-exploitation trade-off\n",
    "            exploration_rate_threshold = random.uniform(0, 1)\n",
    "            if exploration_rate_threshold > exploration_rate:\n",
    "                action = np.argmax(q_table[state,:]) # Exploitation\n",
    "            else:\n",
    "                action = env.action_space.sample() # Exploration\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "            # Sarsa: An on-policy TD control algorithm.\n",
    "            q_table[state, action] = q_table[state, action]  + \\\n",
    "                learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - (q_table[state, action]))\n",
    "\n",
    "            state = new_state\n",
    "            rewards_current_episode += reward\n",
    "\n",
    "\n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "\n",
    "        # Exploration rate decay\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "    # Calculate and print the average reward per thousand episodes\n",
    "    rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "    count = 1000\n",
    "\n",
    "    print(\"********Average reward per thousand episodes********\\n\")\n",
    "    for r in rewards_per_thousand_episodes:\n",
    "        print(count, \": \", str(sum(r/1000)))\n",
    "        count += 1000\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2277c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactoryperiod(q_table,exploration_rate,num_episodes,learning_rate,discount_rate):\n",
    "    \n",
    "    rewards_all_episodes = []\n",
    "\n",
    "    # Q-Learning algorithm\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "        \n",
    "        refactory_period = 5\n",
    "\n",
    "        for step in range(max_steps_per_episode): \n",
    "\n",
    "            # Exploration-exploitation trade-off\n",
    "            exploration_rate_threshold = random.uniform(0, 1)\n",
    "            if exploration_rate_threshold > exploration_rate:\n",
    "                action = np.argmax(q_table[state,:]) # Exploitation\n",
    "            else:\n",
    "                action = env.action_space.sample() # Exploration\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "\n",
    "            if(refactory_period == 0):\n",
    "                # Sarsa: An on-policy TD control algorithm.\n",
    "                q_table[state, action] = q_table[state, action]  + \\\n",
    "                    learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - (q_table[state, action]))\n",
    "                \n",
    "                refactory_period = 5\n",
    "                \n",
    "            refactory_period = refactory_period - 1\n",
    "\n",
    "            state = new_state\n",
    "            rewards_current_episode += reward\n",
    "\n",
    "\n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "\n",
    "        # Exploration rate decay\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "\n",
    "    # Calculate and print the average reward per thousand episodes\n",
    "    rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "    count = 1000\n",
    "\n",
    "    print(\"********Average reward per thousand episodes********\\n\")\n",
    "    for r in rewards_per_thousand_episodes:\n",
    "        print(count, \": \", str(sum(r/1000)))\n",
    "        count += 1000\n",
    "    \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22be18ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.03900000000000003\n",
      "\n",
      "\n",
      "******** Basic Q-table********\n",
      "\n",
      "[[0.26559475 0.23274041 0.24564058 0.24158167]\n",
      " [0.12776628 0.11074493 0.13451671 0.19057206]\n",
      " [0.16974807 0.0758261  0.06986535 0.08831105]\n",
      " [0.02216495 0.02746777 0.02271978 0.07125702]\n",
      " [0.28365454 0.13692225 0.20646591 0.19683609]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04187798 0.00792743 0.18040892 0.00625731]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11663474 0.21402043 0.19327489 0.32659224]\n",
      " [0.11112157 0.40304968 0.21934775 0.13184289]\n",
      " [0.36172744 0.13473909 0.11837375 0.10243724]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.19867333 0.26404278 0.52370948 0.21262982]\n",
      " [0.25497666 0.38137619 0.34084856 0.73639774]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "new_qtable = basic(q_table,exploration_rate,num_episodes,learning_rate,discount_rate)\n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n******** Basic Q-table********\\n\")\n",
    "print(new_qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f58dbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "q_learning(new_qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae7aa8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.04200000000000003\n",
      "\n",
      "\n",
      "******** Sarsa Q-table********\n",
      "\n",
      "[[0.42856725 0.38610366 0.38048682 0.38076119]\n",
      " [0.28767148 0.19595752 0.22958006 0.36009383]\n",
      " [0.29361006 0.25352963 0.26067328 0.28030392]\n",
      " [0.11544219 0.13592306 0.16877386 0.25596025]\n",
      " [0.44065154 0.30514626 0.38550196 0.26240043]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18135853 0.16611273 0.24461295 0.09571862]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.26443513 0.32649613 0.24607827 0.46230674]\n",
      " [0.31721167 0.52497163 0.2877823  0.2489617 ]\n",
      " [0.54851515 0.38440643 0.36083512 0.25527655]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.38624921 0.40801744 0.63507975 0.35843081]\n",
      " [0.54025413 0.73346571 0.64451419 0.74436241]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "new_qtable = sarsa(q_table,exploration_rate,num_episodes,learning_rate,discount_rate)\n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n******** Sarsa Q-table********\\n\")\n",
    "print(new_qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bafc117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "q_learning(new_qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7aa1c64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.05500000000000004\n",
      "\n",
      "\n",
      "******** Sarsa with Refactory Period Q-table********\n",
      "\n",
      "[[0.44574817 0.41889471 0.43751341 0.42512771]\n",
      " [0.29170684 0.26719723 0.28346144 0.37117249]\n",
      " [0.3078411  0.30833751 0.28989709 0.32621517]\n",
      " [0.19489307 0.140767   0.1572782  0.30888375]\n",
      " [0.46448531 0.32032984 0.33051207 0.31907576]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18308925 0.21149345 0.21460231 0.08137745]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37480544 0.32424758 0.33242296 0.5117188 ]\n",
      " [0.30798565 0.53802043 0.41580786 0.33464519]\n",
      " [0.54329851 0.47000757 0.37275719 0.28171276]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.38408118 0.41366457 0.62127065 0.36719586]\n",
      " [0.5859829  0.74232636 0.72671425 0.72818954]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "new_qtable = refactoryperiod(q_table,exploration_rate,num_episodes,learning_rate,discount_rate)\n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n******** Sarsa with Refactory Period Q-table********\\n\")\n",
    "print(new_qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07f70162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "****You fell through a hole!****\n"
     ]
    }
   ],
   "source": [
    "q_learning(new_qtable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
